{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "presidential-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile\n",
    "import librosa\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Dense, BatchNormalization, ReLU, Input, LSTM, Concatenate, Conv2DTranspose, Reshape, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-translation",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "rocky-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SID_component(sid_input):\n",
    "\n",
    "    conv1 = Conv2D(filters=48, kernel_size=(1,7), dilation_rate=(1,1), padding='same')(sid_input)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = ReLU()(conv1)\n",
    "\n",
    "    conv2 = Conv2D(filters=48, kernel_size=(7,1), dilation_rate=(1,1), padding='same')(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = ReLU()(conv2)\n",
    "\n",
    "    conv3 = Conv2D(filters=48, kernel_size=(5,5), dilation_rate=(1,1), padding='same')(conv2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = ReLU()(conv3)\n",
    "\n",
    "    conv4 = Conv2D(filters=48, kernel_size=(5,5), dilation_rate=(2,1), padding='same')(conv3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = ReLU()(conv4)\n",
    "\n",
    "    conv5 = Conv2D(filters=48, kernel_size=(5,5), dilation_rate=(4,1), padding='same')(conv4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = ReLU()(conv5)\n",
    "\n",
    "    conv6 = Conv2D(filters=48, kernel_size=(5,5), dilation_rate=(8,1), padding='same')(conv5)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = ReLU()(conv6)\n",
    "\n",
    "    conv7 = Conv2D(filters=48, kernel_size=(5,5), dilation_rate=(16,1), padding='same')(conv6)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = ReLU()(conv7)\n",
    "\n",
    "    conv8 = Conv2D(filters=48, kernel_size=(5,5), dilation_rate=(32,1), padding='same')(conv7)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = ReLU()(conv8)\n",
    "\n",
    "    conv9 = Conv2D(filters=48, kernel_size=(5,5), dilation_rate=(1,1), padding='same')(conv8)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = ReLU()(conv9)\n",
    "\n",
    "    conv10 = Conv2D(filters=48, kernel_size=(5,5), dilation_rate=(2,2), padding='same')(conv9)\n",
    "    conv10 = BatchNormalization()(conv10)\n",
    "    conv10 = ReLU()(conv10)\n",
    "\n",
    "    conv11 = Conv2D(filters=48, kernel_size=(5,5), dilation_rate=(4,4), padding='same')(conv10)\n",
    "    conv11 = BatchNormalization()(conv11)\n",
    "    conv11 = ReLU()(conv11)\n",
    "\n",
    "    conv12 = Conv2D(filters=8, kernel_size=(1,1), dilation_rate=(1,1), padding='same')(conv11)\n",
    "    conv12 = BatchNormalization()(conv12)\n",
    "    conv12 = ReLU()(conv12)\n",
    "    \n",
    "    lstm_input = Reshape((256*291,8))(conv12)\n",
    "\n",
    "    lstm = Bidirectional(LSTM(units=100))(lstm_input)\n",
    "\n",
    "    fc = Dense(units=100)(lstm)\n",
    "    fc = ReLU()(fc)\n",
    "\n",
    "    sid_output = Dense(units=291)(fc)\n",
    "\n",
    "    return sid_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-russian",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silence_mask_extraction(audio):\n",
    "    audio_stft = librosa.stft(audio, n_fft=510, hop_length=110, win_length=28, window=\"hann\")\n",
    "    return np.abs(librosa.util.normalize(audio_stft)).mean(axis=0) > 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(dirs):\n",
    "    df = pd.DataFrame({dir:listdir(dir) for dir in dirs})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "super-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(noisy_path, clean_path, dataframe, batch_size, size):\n",
    "    \n",
    "    X = np.empty((batch_size,)+size)\n",
    "    Y = np.empty((batch_size,)+size[1])\n",
    "    \n",
    "    n_samples = dataframe.shape[0]\n",
    "    batch_ind = 0\n",
    "    epoch_ind = 0\n",
    "    \n",
    "    while True:\n",
    "        clean_audio, _ = librosa.load(join(clean_path, dataframe.iloc[epoch_ind, 0]), sr=16000)\n",
    "        noisy_audio, _ = librosa.load(join(noisy_path, dataframe.iloc[epoch_ind, 1]), sr=16000)\n",
    "        \n",
    "        if librosa.get_duration(clean_audio, sr=16000) < 2:\n",
    "            continue\n",
    "        \n",
    "        clean_mask = silence_mask_extraction(clean_audio)\n",
    "        noisy_stft = librosa.stft(noisy_audio, n_fft=510, hop_length=110, win_length=28, window=\"hann\")\n",
    "            \n",
    "        X[batch_ind] = noisy_stft\n",
    "        Y[batch_ind] = clean_mask\n",
    "        \n",
    "        batch_ind += 1\n",
    "        epoch_ind += 1\n",
    "        \n",
    "        if batch_ind == batch_size:\n",
    "            yield X, Y\n",
    "            batch_ind = 0\n",
    "            \n",
    "        if epoch_ind == n_samples:\n",
    "            epoch_ind = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "solid-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_splitting(input_filename, input_path, output_path, duration=2):\n",
    "    audio, sr = librosa.load(join(input_path, input_filename), sr=16000)\n",
    "    \n",
    "    buffer = duration * sr\n",
    "\n",
    "    samples_total = len(audio)\n",
    "    samples_wrote = 0\n",
    "    counter = 1\n",
    "\n",
    "    while samples_wrote < samples_total:\n",
    "\n",
    "        #check if the buffer is not exceeding total samples \n",
    "        if buffer > (samples_total - samples_wrote):\n",
    "            buffer = samples_total - samples_wrote\n",
    "\n",
    "        block = audio[samples_wrote : (samples_wrote + buffer)]\n",
    "        temp = input_filename.split('.')\n",
    "        output_filename = join(output_path, temp[0] + '_' + str(counter) + '.' + temp[1])\n",
    "\n",
    "        # Write 2 second segment\n",
    "        soundfile.write(output_filename, block, sr)\n",
    "        counter += 1\n",
    "        samples_wrote += buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "reverse-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/\"\n",
    "\n",
    "base_train_dirs = [\"clean_trainset_28spk_wav\", \"noisy_trainset_28spk_wav\"]\n",
    "base_test_dirs = [\"clean_testset_wav\", \"noisy_testset_wav\"]\n",
    "\n",
    "splitted_train_dirs = [\"splitted_clean_trainset_28spk_wav\", \"splitted_noisy_trainset_28spk_wav\"]\n",
    "splitted_test_dirs = [\"splitted_clean_testset_wav\", \"splitted_noisy_testset_wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "secure-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_path(path, dirs):\n",
    "    return list(map(lambda d: join(path, d), dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "small-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = make_df(make_path(data_path, base_train_dirs))\n",
    "test_df = make_df(make_path(data_path, base_test_dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_dir, splitted_train_dir in zip(base_train_dirs, splitted_train_dirs):\n",
    "    for file in train_df[train_dir]:\n",
    "        audio_splitting(file, join(data_path, train_dir), join(data_path, splitted_train_dir))\n",
    "        \n",
    "for test_dir, splitted_test_dir in zip(base_test_dirs, splitted_test_dirs):\n",
    "    for file in test_df[test_dir]:\n",
    "        audio_splitting(file, join(data_path, test_dir), join(data_path, splitted_test_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "forward-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = join(data_path, splitted_train_dirs[0])\n",
    "noisy_path = join(data_path, splitted_train_dirs[1])\n",
    "\n",
    "spl_train_df = make_df(make_path(data_path, splitted_train_dirs))\n",
    "spl_test_df = make_df(make_path(data_path, splitted_train_dirs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-desire",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 291, 1)\n",
    "batch_size = 15\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "train_generator = data_gen(noisy_path, clean_path, spl_train_df, batch_size, size)\n",
    "valid_generator = data_gen(noisy_path, clean_path, spl_test_df, batch_size, size)\n",
    "\n",
    "steps_per_train_epoch = spl_train_df.shape[0]//batch_size\n",
    "steps_per_valid_epoch = spl_test_df.shape[0]//batch_size\n",
    "\n",
    "inputs = Input(shape=size)\n",
    "outputs = SID_component(inputs)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=Adam(lr=lr),\n",
    "              loss='binary_cross_entropy',\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(train_datagen, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=1,\n",
    "                    validation_data=valid_datagen,\n",
    "                    steps_per_epoch=steps_per_train_epoch,\n",
    "                    validation_steps=steps_per_valid_epoch)\n",
    "\n",
    "\n",
    "model_name = \"sid_1\"\n",
    "\n",
    "model.save(join(data_path, 'model_{}.hdf5'.format(model_name)))\n",
    "with open(join(data_path, 'stats_{}.pickle'.format(model_name)), 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "adaptive-klein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(256, 1368)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,s = librosa.load('data/noisy_trainset_28spk_wav/p226_021.wav', sr=16000)\n",
    "print(librosa.get_duration(y,s))\n",
    "librosa.stft(y, n_fft=510, hop_length=110, win_length=28, window=\"hann\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "composed-termination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 291)\n",
      "(256, 291)\n",
      "(256, 291)\n",
      "(256, 291)\n",
      "(256, 204)\n"
     ]
    }
   ],
   "source": [
    "buffer = 2 * s\n",
    "\n",
    "samples_total = len(y)\n",
    "samples_wrote = 0\n",
    "counter = 1\n",
    "\n",
    "while samples_wrote < samples_total:\n",
    "\n",
    "    #check if the buffer is not exceeding total samples \n",
    "    if buffer > (samples_total - samples_wrote):\n",
    "        buffer = samples_total - samples_wrote\n",
    "\n",
    "    block = y[samples_wrote : (samples_wrote + buffer)]\n",
    "    print(librosa.stft(block, n_fft=510, hop_length=110, win_length=28, window=\"hann\").shape)\n",
    "    counter += 1\n",
    "    samples_wrote += buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-liberal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
